<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.58.3" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://www.cis.upenn.edu/~chrjung/blog/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://www.cis.upenn.edu/~chrjung/blog/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://www.cis.upenn.edu/~chrjung/blog/favicon-16x16.png">

  
  <link rel="manifest" href="https://www.cis.upenn.edu/~chrjung/blog/site.webmanifest">

  
  <link rel="mask-icon" href="https://www.cis.upenn.edu/~chrjung/blog/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://www.cis.upenn.edu/~chrjung/blog/css/bootstrap.min.css" />

  
  <title>The Monitor Argument: 2. Proof Sketch | Chris Jung</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="https://www.cis.upenn.edu/~chrjung/blog/">Posts</a>
    
    <a href="https://www.cis.upenn.edu/~chrjung/blog/tags/">Tags</a>
    
    <a href="https://www.cis.upenn.edu/~chrjung/blog/about/">About</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
    onload="
    renderMathInElement(
          document.body,
          {
              delimiters: [
                  {left: '$$', right: '$$', display: true},
                  {left: '$', right: '$', display: false}
                  // {left: '\\(', right: '\\)', display: false}
              ],
              macros: {
                '\\nl': '\\\\',
                '\\RR': '\\mathbb{R}'
              },
              throwOnError: true
          }
      );
      "></script>






<h1>The Monitor Argument: 2. Proof Sketch</h1>
<p>
  <small class="text-secondary">
  
  
  Oct 15, 2019
  </small>
  

<small><code><a href="https://www.cis.upenn.edu/~chrjung/blog/tags/concentration">concentration</a></code></small>


<small><code><a href="https://www.cis.upenn.edu/~chrjung/blog/tags/stability">stability</a></code></small>


<small><code><a href="https://www.cis.upenn.edu/~chrjung/blog/tags/differential-privacy">differential privacy</a></code></small>


<small><code><a href="https://www.cis.upenn.edu/~chrjung/blog/tags/adaptive-data-analysis">adaptive data analysis</a></code></small>

</p>


<p>In <a href="https://www.cis.upenn.edu/~chrjung/blog/monitor-argument">the previous post</a>, we went over three different settings and saw how they can all be understood under one unifying framework. In this post, we will first go over the high level proof sketch, and then we will prove all the lemmas that are not specific to the setting.</p>

<p><br>
<br></p>

<h4 id="proof-sketch">Proof Sketch</h4>

<p>Here&rsquo;s the statement that we want to prove:
$$
\Pr_{S \sim \mathcal{P}^n, q = M(S)}\left( |q(S) - E_{S \sim \mathcal{P}^n} \left[ q(S) \right] | \ge \alpha \right) \le \beta.
$$</p>

<p>We&rsquo;ll proceed with proof by contradiction: suppose for the sake of contradiction that the above concentration doesn&rsquo;t hold for some $\alpha$ and $\beta$.</p>

<p>$$
\tag{1} \Pr_{S \sim \mathcal{P}^n, q = M(S)}( |q(S) - E_{S \sim \mathcal{P}^n} \left[ q(S) \right] | \ge \alpha ) &gt; \beta.
$$</p>

<p>Now, we will construct a monitor $W$ and perform the following thought experiment. Take $m$ independent sample sets, $S_1, \dots, S_m$, and let it go through $M$ independently. This will produce $q_1(S_1), \dots, q_m(S_m)$. Assume that the monitor $W$ actually has access to the underlying distribution $\mathcal{P}$; normally, we don&rsquo;t have access to the distribution $\mathcal{P}$, and this is precisely why we refer to this as a thought experiment. Hence, the monitor can actually find $j \in [m]$ whose answer is furthest apart from its expectation $E_{S}[q_j(S)]$ &ndash; in other words, $W$ may output $\argmax_{j \in [m]} |q_j(S_j) - E_{S}\left[ q_j(S) \right]|$. However, instead of exactly outputting the index $j$ with the biggest difference from its expected value, we will have each index be chosen with probability exponentially proportional to the difference. That is,
$$\Pr( W(S_1, \dots, S_m) = j) \propto e^{\frac{c}{2\Delta} |q_j(S_j) - E_{S}\left[ q_j(S) \right]|},$$
where $c$ is some parameter we can choose and $\Delta$ is the sensitivity of the queries &ndash; that is, for any neighboring dataset $S$ and $S&rsquo;$, $|q(S) - q(S&rsquo;)| \le \Delta$. We will write $W_c$ whenever we want to surface the parameter $c$.
Note that the index with the biggest difference is still most likely to be chosen, but we are still allowing for other possibility for other indices to be chosen here. We&rsquo;ll discuss why we have the monitor output the index in this &lsquo;stable&rsquo; way instead of just outputting the most violated index. Let $j^* = W(S_1, \dots, S_m)$ denote the index that is output by the monitor $W$.</p>

<!-- In order to ignore the absolute value, with probability proportional to $e^{\frac{c}{2} (q\_j(S\_j) - E\_{S}\left[ q\_j(S) \right])}$, $j$ will be sampled, and also, with probability proportional to $e^{\frac{c}{2} (E\_{S}\left[ q\_j(S) \right] - q\_j(S\_j))}$, $j$ will be sampled. -->

<p>Now, our goal will be to derive a contradiction by lower-bounding and upper-bounding the expected difference for $j^*$.</p>

<p>$$ B_L &lt; E_{S_1, \dots, S_m, W}[|q_{j^* }(S_{j^* }) - E_{S} \left[ q_{j^* }(S)| \right]] &lt; B_U$$ where $B_L &gt; B_U$.</p>

<p><img src="../img/Monitor.png"></p>

<p>To derive the bounds, our strategy will be as follows:</p>

<ol>
<li>Using (1), we can show the lower-bound $B_L$ can&rsquo;t be too small.</li>
<li>Using the stability of $M$ and $W$, we can show that $B_U$ can&rsquo;t be too big.</li>
</ol>

<p>The only part that is specific to each application is showing that $B_U$ has to be sufficiently small, as that will need to leverage how $M$&rsquo;s stability is defined.</p>

<p><br>
<br></p>

<h4 id="lemmas">Lemmas</h4>

<p>Once again, our goal here is to bound $B_L$. We will first lower bound the maximum difference $D_{max} := \max_{j \in [m]} |q_j(S_j) - E_{S}\left[ q_j(S) \right]|$ through our contradiction assumption in (1). Then, we will show that the monitor $W$ outputs $j^*$ whose difference is pretty close to $D_{max}$, thereby bounding $B_L$ in expectation.</p>

<h5 id="lemma-1-if-inequality-1-is-true-e-w-s-1-dots-s-m-left-d-max-right-alpha-left-1-left-1-beta-right-m-right">Lemma 1. If inequality (1) is true, $$E_{W, S_1, \dots, S_m}\left[D_{max}\right] &gt; \alpha \left( 1 - \left(1 - \beta\right)^m \right)$$</h5>

<p><details>
    <summary>Proof: (click here)</summary></p>

<p>From (1), we have that with probability at most $1-\beta$, $|q_j(S_j) - E_{S}\left[ q_j(S) \right]| &lt; \alpha$ for any $j \in [m]$. Because each $j$th run in the monitor is independent of each other, we have that with probability $\left(1 - \beta\right)^m$,
$$ |q_j(S_j) - E_{S}\left[ q_j(S) \right]| &lt; \alpha, \forall j \in [m].$$
In other words, with probability $1-\left(1 - \beta\right)^m$ there exists at least some $j&rsquo; \in [m]$ whose difference is more than $\alpha$. Therefore, in expectation, the max difference must be more than
$\alpha \left( 1 - \left(1 - \beta\right)^m \right).$</p>

<p>$\square$
</details></p>

<p><br/></p>

<p><br></p>

<p>Now, we show that in expectation, $j^*$ has a difference not too mush smaller than $D_{max}$.  The following lemma simply follows from the fact that <a href="https://en.wikipedia.org/wiki/Exponential_mechanism_(differential_privacy)">exponential mechanism</a> preserves utility up to some additive error in expectation.</p>

<h5 id="lemma-2-fix-c-s-1-dots-s-i-dots-s-m-and-q-j-m-s-j-for-all-j-in-m-also-let-delta-be-the-sensitivity-of-the-queries-q-j-s-q-j-s-le-delta-for-any-neighboring-dataset-s-and-s-then-we-have-e-w-c-q-j-s-j-e-s-q-j-s-c-lbrace-q-j-rbrace-j-1-m-ge-d-max-frac-2-delta-ln-m-c">Lemma 2. Fix $C = (S_1, \dots, S_i, \dots, S_m)$ and $q_j = M(S_j)$ for all $j \in [m]$. Also, let $\Delta$ be the sensitivity of the queries: $|q_j(S) - q_j(S&rsquo;)| \le \Delta$ for any neighboring dataset $S$ and $S&rsquo;$. Then, we have $$E_{W_{c}}[|q_{j^*}(S_{j^*}) - E_S [ q_{j^*}(S)] | | C, \lbrace q_j \rbrace_{j=1}^m] \ge D_{max} - \frac{2 \Delta \ln m}{c}.$$</h5>

<p><details>
    <summary>Proof: (click here)</summary>
    <div>
    </div>
During the proof, we&rsquo;ll suppress the conditional term $\Pr(\cdot | C, \lbrace q_j \rbrace_{j=1}^m)$ for notational simplicity.</p>

<p>We have that $$\Pr\left(W_c ( C ) = j \right) = \frac{1}{K} e^{\frac{c}{2\Delta} |q_j(S_j) - E_{S}\left[ q_j(S) \right]|},$$ where $K=\sum_{j=1}^m e^{\frac{c}{2\Delta} |q_j(S_j) - E_{S}\left[ q_j(S) \right]|}$.</p>

<p>Rearranging some terms and suppressing the conditional term just for notation simplicity, we get
$$ |q_j(S_j) - E_{S}\left[ q_j(S) \right]| = \frac{2\Delta}{c}\left( \ln K + \ln \Pr\left(W_c ( C ) = j\right) \right).$$</p>

<p>Now, consider the expected difference for $j^*$.
$$
\begin{aligned}
E_{W_{c}}[|q_{j^*}(S_{j^*}) - E_S [ q_{j^*}(S)] |] &amp;= \sum_{j=1}^m \Pr(W_c = j) \cdot |q_{j}(S_{j}) - E_S [ q_{j}(S)] | \nl
&amp;= \sum_{j=1}^m \Pr(W_c = j) \cdot \frac{2\Delta}{c}\left( \ln K + \ln \Pr\left(W_c ( C ) = j\right) \right) \nl
&amp;= \frac{2\Delta \ln K }{c}  + \frac{2\Delta}{c} \sum_{j=1}^m \Pr(W_c = j) \cdot \ln \Pr\left(W_c ( C ) = j\right) \nl
&amp;\ge \frac{2\Delta \ln K }{c}  - \frac{2\Delta \ln m}{c},
\end{aligned}
$$
where the last inequality follows from the fact that Shannon entropy of a set of size $m$ is at most $\ln m$: $\sum_{j=1}^m \Pr(W_c = j) \cdot -\ln \Pr\left(W_c ( C ) = j\right) \le \ln(m)$.</p>

<p>Finally, note that $$\ln K  = \ln \sum_{j=1}^m e^{\frac{c}{2\Delta} |q_j(S_j) - E_{S}\left[ q_j(S) \right]|} \ge \ln \max_{j\in [m]}  e^{\frac{c}{2\Delta} |q_j(S_j) - E_{S}\left[ q_j(S) \right]|} \ge \max_{j \in [m]} \frac{c}{2\Delta} |q_{j}(S_{j}) - E_S [ q_{j}(S)] |.$$</p>

<p>Therefore, $E_{W_{c}}[|q_{j^*}(S_{j^*}) - E_S [ q_{j^*}(S)] |] &gt; D_{max} - \frac{2\Delta \ln m}{c}.$</p>

<p>$\square$</p>

<p></details></p>

<p><br></p>

<p>Combining Lemma 2 and 3, we can show the lower bound $B_L$.</p>

<h5 id="lemma-3-if-inequality-1-is-true-alpha-left-1-left-1-beta-right-m-right-frac-2-delta-ln-m-c-e-s-1-dots-s-m-w-q-j-s-j-e-s-left-q-j-s-right">Lemma 3. If inequality (1) is true, $$ \alpha \left( 1 - \left(1 - \beta\right)^m \right) - \frac{2 \Delta \ln m}{c} &lt; E_{S_1, \dots, S_m, W}[|q_{j^* }(S_{j^* }) - E_{S} \left[ q_{j^* }(S)| \right]] $$</h5>

<p><details>
    <summary>Proof: (click here)</summary></p>

<p>$$
\begin{aligned}
 E_{S_1, \dots, S_m, M, W}[|q_{j^* }(S_{j^* }) - E_{S} \left[ q_{j^* }(S)| \right]] &amp;&gt; D_{max} - \frac{2\Delta\ln m}{c} \nl
 &amp;&gt; \alpha \left( 1 - \left(1 - \beta\right)^m \right) - \frac{2\Delta\ln m}{c}
\end{aligned}
$$
Because Lemma 2 holds for any fixed $C = (S_1, \dots, S_i, \dots, S_m)$ and $q_j = M(S_j)$ for all $j \in [m]$, the inequality must hold true in expectation as well.</p>

<p>$\square$
</details></p>

<p></br></p>

<p>Lastly, we show that $W_c$ is $c$-differentially private; this lemma is just showing that exponential mechanism is differentially private.</p>

<h5 id="lemma-4-fix-c-s-1-dots-s-i-dots-s-m-and-tilde-c-tilde-s-1-dots-dots-tilde-s-m-where-s-j-and-tilde-s-j-differ-in-at-most-one-element-also-fix-q-j-m-s-j-for-all-j-in-m">Lemma 4. Fix $C = (S_1, \dots, S_i, \dots, S_m)$ and $\tilde{C} = (\tilde{S}_1, \dots, \dots, \tilde{S}_m)$, where $S_j$ and $\tilde{S}_j$ differ in at most one element. Also, fix $q_j = M(S_j)$ for all $j \in [m]$.</h5>

<h5 id="then-we-have-pr-left-w-c-c-j-c-lbrace-q-j-rbrace-j-1-m-right-le-e-c-pr-left-w-c-tilde-c-j-tilde-c-lbrace-q-j-rbrace-j-1-m-right">Then, we have $$ \Pr\left(W_c ( C ) = j | C, \lbrace q_j \rbrace_{j=1}^m  \right) \le e^{c} \Pr\left(W_c(\tilde{C}) = j |  \tilde{C}, \lbrace q_j \rbrace_{j=1}^m  \right)$$</h5>

<p><details>
    <summary>Proof: (click here)</summary></p>

<p>For any $j$,
$$
\begin{aligned}
&amp;\frac{\exp(\frac{c}{2\Delta} |q_{j}(S_{j}) - E_S [ q_{j}(S)] |)}{\exp(\frac{c}{2\Delta} |q_{j}(\tilde{S}_{j}) - E_S [ q_{j}(S)]|)} \nl
&amp;\le \exp(\frac{c}{2\Delta} |q_{j}(S_{j}) - q_{j}(\tilde{S}_{j})| ) \nl
&amp;= \exp(\frac{c}{2}).
\end{aligned}
$$</p>

<p>Also, define $\tilde{K} = \sum_{j=1}^m e^{\frac{c}{2\Delta} |q_j(\tilde{S}_j) - E_{S}\left[ q_j(S) \right]|}$.
$$
\begin{aligned}
\tilde{K}=\sum_{j=1}^m e^{\frac{c}{2\Delta} |q_j(\tilde{S}_j) - E_{S}\left[ q_j(S) \right]|} \le \sum_{j=1}^m e^{\frac{c}{2}} e^{\frac{c}{2\Delta} |q_j(S_j) - E_{S}\left[ q_j(S) \right]|} = \exp(\frac{c}{2}) K.
\end{aligned}
$$</p>

<p>Combining the inequalities together, we get
$$
\frac{\Pr\left(W_c ( C ) = j | C, \lbrace q_j \rbrace_{j=1}^m  \right)}{\Pr\left(W_c(\tilde{C}) = j |  \tilde{C}, \lbrace q_j \rbrace_{j=1}^m  \right)} \le \frac{\exp(\frac{c}{2\Delta} |q_{j}(S_{j}) - E_S [ q_{j}(S)] |)}{\exp(\frac{c}{2\Delta} |q_{j}(\tilde{S}_{j}) - E_S [ q_{j}(S)]|)} \cdot \frac{\tilde{K}}{K} \le \exp( c ).
$$</p>

<p>$\square$
</details></p>

<p><br/>
<br/></p>

<h4 id="chernoff-bound-adaptive-data-analysis">Chernoff Bound &amp; Adaptive Data Analysis</h4>

<p>We&rsquo;ll finish off the proof for the case when $M$ is $(\epsilon, \delta)$-differentially private. Note that for the Chernoff bound case, because $M$ always outputs the same query, $q_{avg}$, $M$ is essentially $(0,0)$-differentially private.</p>

<p>For the purpose of simplicity, let&rsquo;s only consider statistical queries, meaning each $q(S) = \sum_{i=1}^n q(x_i)$. Note that the sensitivity of statistical queries is $\frac{1}{n}$.</p>

<h5 id="lemma-5-if-m-is-epsilon-delta-differentially-private-then">Lemma 5. If $M$ is $(\epsilon, \delta)$-differentially private, then</h5>

<h5 id="e-s-1-dots-s-m-j-w-c-s-1-dots-s-m-q-j-s-j-e-s-q-j-s-le-e-c-epsilon-1-m-delta">$$E_{S_1, \dots, S_m, j^*=W_c(S_1, \dots, S_m)}[ |q_{j^*}(S_{j^*}) - E_{S}[q_{j^*}(S)|] \le e^{c+\epsilon}-1 + m \delta$$</h5>

<p><details>
    <summary>Proof: (click here)</summary>
Let $S_j(i)$ denote $i$th element of $S_j$. Then, we have
$$
\begin{aligned}
&amp;E_{S_1, \dots, S_m, j^*=W_c( C )}[q_{j^*}(S_{j^*})] \nl
&amp;= \sum_{C, \lbrace q_j \rbrace_{j=1}^m} \Pr( C ) \cdot \Pr(\lbrace q_j \rbrace_{j=1}^m | C) \cdot \sum_{j=1}^m \Pr(W_c ( C ) = j | C, \lbrace q_j \rbrace_{j=1}^m) \cdot \frac{1}{n} \sum_{i=1}^n q_j(S_j(i)) \nl
&amp;= \frac{1}{n} \sum_{C, x, \lbrace q_j \rbrace_{j=1}^m} \sum_{j=1}^m \sum_{i=1}^n \Pr( C, x ) \cdot \Pr(\lbrace q_j \rbrace_{j=1}^m | C) \cdot  \Pr(W_c ( C ) = j | C, \lbrace q_j \rbrace_{j=1}^m) \cdot   q_j(S_j(i)),
\end{aligned}
$$
where $x \in \mathcal{X}$ is just an element independently drawn from $\mathcal{P}$.</p>

<p>Let&rsquo;s introduce a new notation, $C_{(i,j) \to x}$, which is essentially $C$ except $i$th element in the $j$th sample set is swapped with $x$. $$C_{(i,j) \to x} := (S_1, \dots, \tilde{S}_j, \dots, S_m)$$ where $\tilde{S}_j(a) = S_j(a)$ for $a \in [n]$ except that $x$ is swapped in for the $i$th element.</p>

<p>Now, let&rsquo;s take a look at the summand
$$
\begin{aligned}
&amp;\Pr( C, x ) \cdot \Pr(\lbrace q_j \rbrace_{j=1}^m | C) \cdot  \Pr(W_c ( C ) = j | C, \lbrace q_j \rbrace_{j=1}^m) \cdot   q_j(S_j(i)) \nl
&amp;\le \Pr( C, x ) \cdot (e^\epsilon \Pr(\lbrace q_j \rbrace_{j=1}^m | C_{(i,j) \to x}) + \delta) \cdot  \Pr(W_c ( C ) = j | C, \lbrace q_j \rbrace_{j=1}^m) \cdot   q_j(S_j(i)) \nl
&amp;\le \Pr( C, x ) \cdot e^\epsilon \Pr(\lbrace q_j \rbrace_{j=1}^m | C_{(i,j) \to x})  \cdot  \Pr(W_c ( C ) = j | C, \lbrace q_j \rbrace
_{j=1}^m) \cdot   q_j(S_j(i)) + \delta\nl
&amp;\le \Pr( C, x ) \cdot e^{\epsilon+c} \Pr(\lbrace q_j \rbrace_{j=1}^m | C_{(i,j) \to x})  \cdot \Pr(W_c ( C_{(i,j) \to x} ) = j | C_{(i,j) \to x}, \lbrace q_j \rbrace_{j=1}^m) \cdot   q_j(S_j(i)) + \delta,
\end{aligned}
$$
where the first inequality comes from the fact that $M$ is $(\epsilon, \delta)$-differentially private and the last inequality is due to lemma 4.</p>

<p>Plugging this back in, we get
$$
\begin{aligned}
&amp;E_{S_1, \dots, S_m, j^*=W_c( C )}[q_{j^*}(S_{j^*})] \nl
&amp;\le \frac{1}{n} e^{\epsilon + c} \sum_{j=1}^m \sum_{i=1}^n E_{C, x, W_c}[\mathbf{1}(W_c(C_{(i,j) \to x}) = j) \cdot q_j(S_j(i))] + m\delta \nl
\end{aligned}
$$</p>

<p>Here&rsquo;s one of my favorite tricks! Note that $(C,x)$ is identitically distributed as $(C_{(i,j) \to x}, S_j(i))$, so the above value is equivalent to</p>

<p>$$
\begin{aligned}
&amp;=\frac{1}{n} e^{\epsilon + c} \sum_{j=1}^m \sum_{i=1}^n E_{C, x, W_c}[\mathbf{1}(W_c(C = j) \cdot q_j(x)] + m\delta \nl
&amp;= e^{\epsilon + c} E_{C, x, W_c}[q_j(x)] + m\delta = e^{\epsilon + c} E_{S}[q_j(S)] + m\delta
\end{aligned}
$$</p>

<p>The same argument can be used to prove the opposite direction as well. Therefore, we get
$$E_{S_1, \dots, S_m, j^*=W_c(S_1, \dots, S_m)}[|q_{j^*}(S_{j^*}) - E_{S}[q_{j^*}(S)|] \le e^{\epsilon + c}-1 + m \delta$$</p>

<p>$\square$</p>

<p></details></p>

<p><br>
Now, here&rsquo;s our long awaited theorem! You can refernece <a href="https://adaptivedataanalysis.files.wordpress.com/2017/11/lect14.pdf">these notes</a> as to how I set some of the constants here.</p>

<h5 id="theorem-1-if-m-is-epsilon-delta-differentially-private-for-epsilon-le-frac-1-5-then">Theorem 1. If $M$ is $(\epsilon, \delta)$-differentially private for $\epsilon \le \frac{1}{5}$, then</h5>

<h5 id="pr-s-sim-mathcal-p-n-q-m-s-left-q-s-e-s-sim-mathcal-p-n-left-q-s-right-ge-alpha-right-le-max-frac-4-delta-epsilon-e-frac-epsilon-2-n-8">$$ \Pr_{S \sim \mathcal{P}^n, q = M(S)}\left( |q(S) - E_{S \sim \mathcal{P}^n} \left[ q(S) \right] | \ge \alpha \right) \le \max(\frac{4\delta}{\epsilon}, e^\frac{-\epsilon^2 n}{8}). $$</h5>

<p><details>
    <summary>Proof: (click here)</summary>
Combining lemma 3 and lemma 5, we get
$$
\alpha \left( 1 - \left(1 - \beta\right)^m \right) - \frac{2\Delta\ln m}{c} &lt; E_{S_1, \dots, S_m, W}[|q_{j^* }(S_{j^* }) - E_{S} \left[ q_{j^* }(S)| \right]] \le e^{c+\epsilon}-1 + m \delta
$$</p>

<p>Set $\alpha=6\epsilon$, $c=\epsilon$, and $m = \frac{1}{\beta}$. Also, since we are considering statistical queries, $\Delta = \frac{1}{n}$</p>

<p>In that case, the left hand side becomes
$$
\begin{aligned}
\alpha \left( 1 - \left(1 - \beta\right)^m \right) - \frac{2\Delta\ln m}{c} &amp;&lt; e^{c+\epsilon}-1 + m \delta \nl
6\epsilon \left( 1 - \left(1 - \beta\right)^{m} \right) - \frac{2\ln \frac{1}{\beta}}{n \epsilon} &amp;&lt; e^{2\epsilon}-1 + \frac{\delta}{\beta}
\tag{$*$} \nl
6\epsilon - 4\frac{\ln(\frac{1}{\beta})}{\epsilon n} &amp;&lt;  2(e^{2\epsilon}-1) + \frac{2\delta}{\beta} \nl
6\epsilon - 2(e^{2\epsilon}-1)  &amp;&lt; 4\frac{\ln(\frac{1}{\beta})}{\epsilon n}  + \frac{2\delta}{\beta} \nl
\epsilon &amp;&lt; 4\frac{\ln(\frac{1}{\beta})}{\epsilon n}  + \frac{2\delta}{\beta}.
\end{aligned}
$$</p>

<p>For $(*)$, we used the fact that $\left( 1 - \left(1 - \beta\right)^{m} \right) \ge \frac{1}{2}$ because $\left(1 - \beta\right)^{\frac{1}{\beta}} \le e^{-\beta m} \le \frac{1}{2}$ for $\beta &lt; \frac{1}{4}$. As for the last inequality, we used the fact that $\epsilon \le 6\epsilon - 2(e^{2\epsilon}-1)$ for $\epsilon &lt; \frac{1}{5}$.</p>

<p>Now, either $4\frac{\ln(\frac{1}{\beta})}{\epsilon n}$ or $\frac{2\delta}{\beta}$ must be greater than $\frac{\epsilon}{2}$. In other words, $\beta &lt; \max(\frac{4\delta}{\epsilon}, e^\frac{-\epsilon^2 n}{8})$.</p>

<p>Therefore, there&rsquo;s a contradiction if $\beta \ge \max(\frac{4\delta}{\epsilon}, e^\frac{-\epsilon^2 n}{8})$.</p>

<p>$\square$
</details></p>

<p><br>
<br></p>

<h4 id="uniformly-stable-algorithms">Uniformly stable algorithms</h4>

<p>Remember in this case, $q_{f}(S) = L(f, S)$, where $f$ is the classifier returned by the algorithm, and $L$ measures the empirical error on $S$.</p>

<p>Before in the Chernoff bound and adaptive analysis case, we could fix the queries $q$ even as we change the underlying multi-dataset $C$ by one element because differentially private mechanisms are random and the probability of getting the same query doesn&rsquo;t change by much. However, in this case, the queries are deterministically determined as a function of $C$. Hence, if we change $C$, we necessarily have to change the queries as well and can&rsquo;t just analyze some other fixed queries. However, it can be shown that even as we change an element of $C$, the difference $|q_j(S_j) - E_{S}\left[ q_j(S) \right]|$ has a sensitivity of $2\gamma + \frac{1}{n}$. Hence, a similar version of lemma 2 and lemma 3 can be shown for this case. So, in terms its utility, we have
$$
E_{C}[\max_{j \in [m]}|q_j(S_j) - E_{S}\left[ q_j(S) \right]|] \le E_{C}[E_{j^* = W_c( C )}[|q_j(S_j) - E_{S}\left[ q_j(S) \right]|] + \frac{2 (2\gamma + \frac{1}{n})}{c} \ln(m),
$$
and we have that given $C$, $W_c$ is still private.</p>

<p><br></p>

<p>Now, we can employ the same swapping technique as in lemma 5. It&rsquo;s just that in this case, there&rsquo;s an additional cost $\gamma$ in this case.</p>

<h5 id="lemma-7-if-the-algorithm-has-a-uniform-stability-of-gamma-then">Lemma 7. If the algorithm has a uniform stability of $\gamma$, then</h5>

<h5 id="e-s-1-dots-s-m-j-w-c-s-1-dots-s-m-q-j-s-j-e-s-q-j-s-le-e-c-1-gamma">$$E_{S_1, \dots, S_m, j^*=W_c(S_1, \dots, S_m)}[|q_{j^*}(S_{j^*}) - E_{S}[q_{j^*}(S)]|] \le e^{c}-1 + \gamma$$</h5>

<p><details>
    <summary>Proof: (click here)</summary></p>

<p>We introduce a similar swapping notation as lemma 5. Let $(S \to (i,x))$ denote a new sample set where the $i$th element is swapped with $x$.</p>

<p>$$
\begin{aligned}
&amp;E_{S_1, \dots, S_m, j^*=W_c( C )}[q_{j^*}(S_{j^*})] \nl
&amp;= \sum_{C} \Pr( C ) \cdot \sum_{j=1}^m \Pr(W_c( C ) = j) \cdot q_{Alg(S_j)}(S_j) \nl
&amp;= \sum_{C,x} \Pr( C, x) \cdot \sum_{j=1}^m \Pr(W_c( C ) = j) \cdot \frac{1}{n}\sum_{i=1}^n q_{Alg(S_j)}(S_j(i)) \nl
&amp;= \frac{1}{n} \sum_{C,x}  \sum_{j=1}^m  \sum_{i=1}^n \Pr( C, x) \cdot \Pr(W_c( C ) = j | C)  \cdot q_{Alg(S_j)}(S_j(i)) \nl
&amp;\le \sum_{C,x}  \sum_{j=1}^m  \sum_{i=1}^n \Pr( C, x) \cdot e^c \Pr(W_c( C_{(i,j) \to x} ) = j | C_{(i,j) \to x}) \cdot q_{Alg(S_j)}(S_j(i)) \nl
&amp;\le \sum_{C,x}  \sum_{j=1}^m  \sum_{i=1}^n \Pr( C, x) \cdot e^c \Pr(W_c( C_{(i,j) \to x} ) = j | C_{(i,j) \to x}) \cdot (q_{Alg(S \to (i,x))}(S_j(i)) + \gamma) \nl
&amp;= \frac{1}{n}\sum_{j=1}^m \sum_{i=1}^n e^c E_{S_1, \dots, S_m, x}[\mathbf{1}(W_c( C_{(i,j) \to x})=j) \cdot q_{Alg(S \to (i,x))}(x)) + \gamma] \nl
\end{aligned}
$$
where $x \in \mathcal{X}$ is just an element independently drawn from $\mathcal{P}$. Note that unlike before, the query here is chosen deterministically as we change an element in the multi-dataset $C$ within the monitor.</p>

<p>Once again, note that $(C,x)$ is identitically distributed as $(C_{(i,j) \to x}, S_j(i))$, so the above value is equivalent to</p>

<p>$$
\begin{aligned}
=e^c(E_{S_1, \dots, S_m, x, j^*=W_c( C )}[q_{j^*}(x)] + \gamma)
=e^c(E_{S_1, \dots, S_m, j^*=W_c( C )}[ E_{S}[q_{j^*}(S)] + \gamma)
\end{aligned}
$$</p>

<p>The same argument can be used to prove the opposite direction as well.
Therefore, we get
$$E_{S_1, \dots, S_m, j^*=W_c(S_1, \dots, S_m)}[|q_{j^*}(S_{j^*}) - E_{S}[q_{j^*}(S)]|] \le e^{c}-1 + \gamma$$</p>

<p>$\square$
</details></p>

<p><br></p>

<p>Combining these together, we get</p>

<h5 id="theorem-2-if-alg-is-gamma-uniformly-stable-then">Theorem 2. If $Alg$ is $\gamma$-uniformly stable, then</h5>

<h5 id="pr-s-sim-mathcal-p-n-f-alg-s-left-l-f-s-e-s-sim-mathcal-p-n-left-l-f-s-right-8-sqrt-left-2-gamma-frac-1-n-right-cdot-ln-left-frac-8-beta-right-right-le-beta">$$\Pr_{S \sim \mathcal{P}^n, f = Alg(S)}\left(|L(f, S) - E_{S \sim \mathcal{P}^n} \left[ L(f, S) \right] | &gt; 8 \sqrt{\left(2\gamma + \frac{1}{n}\right) \cdot \ln\left(\frac{8}{\beta}\right) } \right) \le \beta$$</h5>

<p><details>
    <summary>Proof: (click here)</summary></p>

<p>Here, unlike before, let&rsquo;s actually fix $\beta$ and find out which $\alpha$ gives rise to contradiction.</p>

<p>Combining the lower bound and the upper bound, we get
$$
\alpha \left( 1 - \left(1 - \beta\right)^m \right) - \frac{2 (2\gamma + \frac{1}{n})}{c} \ln(m) &lt; E_{S_1, \dots, S_m, W}[|q_{j^* }(S_{j^* }) - E_{S} \left[ q_{j^* }(S)| \right]] \le e^{c}-1 + \gamma
$$</p>

<p>Further, reorganizing some terms and setting $m=\frac{e\ln(2)}{\beta}$, $c=\sqrt{(2\gamma + \frac{1}{n}) \cdot \ln(\frac{e\ln(2)}{\beta})}$, we get
$$
\begin{aligned}
\alpha \left( 1 - \left(1 - \beta\right)^m \right) - \frac{2 (2\gamma + \frac{1}{n})}{c} \ln(m) &amp;&lt; e^{c}-1 + \gamma \nl
\alpha \left( 1 - \left(1 - \beta\right)^m \right)  &amp;&lt; \frac{2 (2\gamma + \frac{1}{n})}{c} \ln(m) + e^{c}-1 + \gamma \nl
\alpha (1-e^{-\beta m})  &amp;&lt; \frac{2 (2\gamma + \frac{1}{n})}{c} \ln(m) + e^{c}-1 + \gamma \nl
\alpha (1-e^{\ln(2)})  &amp;&lt; \frac{2 (2\gamma + \frac{1}{n})}{c} \ln(m) + e^{c}-1 + \gamma \nl
 \frac{\alpha}{2}  &amp;&lt; \frac{2 (2\gamma + \frac{1}{n})}{c} \ln(m) + e^{c}-1 + \gamma \nl
 \alpha  &amp;&lt; \frac{4 (2\gamma + \frac{1}{n})}{c} \ln(m) + 2(e^{c}-1 + \gamma) \nl
\end{aligned}
$$</p>

<p>Note that $\alpha \ge 2c$ so the bound holds trivially when $c \ge \frac{1}{2}$. Otherwise, $(e^c - 1) \le 2c$. Therefore,</p>

<p>$$
\alpha &lt; 8 \sqrt{\left(2\gamma + \frac{1}{n}\right) \cdot \ln\left( \frac{e\ln(2)}{\beta} \right)} \le 8 \sqrt{\left(2\gamma + \frac{1}{n}\right) \cdot \ln\left( \frac{8}{\beta} \right)}.
$$</p>

<p>because $\gamma \le \sqrt{\gamma}$.</p>

<p>Therefore, we reach contradiction if $\alpha$ is greater than or equal to the value above.</p>

<p>$\square$
</details></p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-123514321-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
</body>

</html>
