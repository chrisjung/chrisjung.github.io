<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.58.3" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://www.cis.upenn.edu/~chrjung/blog/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://www.cis.upenn.edu/~chrjung/blog/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://www.cis.upenn.edu/~chrjung/blog/favicon-16x16.png">

  
  <link rel="manifest" href="https://www.cis.upenn.edu/~chrjung/blog/site.webmanifest">

  
  <link rel="mask-icon" href="https://www.cis.upenn.edu/~chrjung/blog/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://www.cis.upenn.edu/~chrjung/blog/css/bootstrap.min.css" />

  
  <title>The Monitor Argument: 1. Overview &amp; Applications | Chris Jung</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="https://www.cis.upenn.edu/~chrjung/blog/">Posts</a>
    
    <a href="https://www.cis.upenn.edu/~chrjung/blog/tags/">Tags</a>
    
    <a href="https://www.cis.upenn.edu/~chrjung/blog/about/">About</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
    onload="
    renderMathInElement(
          document.body,
          {
              delimiters: [
                  {left: '$$', right: '$$', display: true},
                  {left: '$', right: '$', display: false}
                  // {left: '\\(', right: '\\)', display: false}
              ],
              macros: {
                '\\nl': '\\\\',
                '\\RR': '\\mathbb{R}'
              },
              throwOnError: true
          }
      );
      "></script>






<h1>The Monitor Argument: 1. Overview &amp; Applications</h1>
<p>
  <small class="text-secondary">
  
  
  Oct 9, 2019
  </small>
  

<small><code><a href="https://www.cis.upenn.edu/~chrjung/blog/tags/concentration">concentration</a></code></small>


<small><code><a href="https://www.cis.upenn.edu/~chrjung/blog/tags/stability">stability</a></code></small>


<small><code><a href="https://www.cis.upenn.edu/~chrjung/blog/tags/differential-privacy">differential privacy</a></code></small>


<small><code><a href="https://www.cis.upenn.edu/~chrjung/blog/tags/adaptive-data-analysis">adaptive data analysis</a></code></small>

</p>


<p><br/>
In this post, I want to go over the Monitor argument introduced by <a href="https://arxiv.org/abs/1511.02513">Bassily et al.</a>; it was originally developed in order to show strong generalization bounds for differentially private algorithms even in the face of adaptivity.</p>

<p>I first learned about the Monitor argument when I took <a href="https://adaptivedataanalysis.com/">Aaron&rsquo;s Adaptive Data Analysis class</a> back in Fall 2017 but have forgotten about it since then. However, when I recently read <a href="https://arxiv.org/abs/1701.03493">Steinke et al.</a> and <a href="https://arxiv.org/abs/1812.09859">Feldman et al.</a> that used the same monitor argument, I was quite struck by the techinque&rsquo;s versatility despite its simplicity (the proofs are actually not that simple, but the high level idea is pretty simple). So, by writing a blog post about it, I hope to retain much of its insights instead of forgetting about it like last time. Also, I&rsquo;m planning on doing my WPE-II on these papers, so hopefully writing this blog will help me organize my thoughts.</p>

<p>The overall structure of this series will be divided as follows:</p>

<h5 id="1-overview-applications-monitor-argument">1. <a href="https://www.cis.upenn.edu/~chrjung/blog/monitor-argument">Overview &amp; Applications</a></h5>

<p>Here, I will talk about three different settings where the monitor argument can be used to prove a generalization bound: Chernoff bound, adaptive data analysis, and uniformly stable algorithms.</p>

<h5 id="2-proofs-monitor-argument-proof">2. <a href="https://www.cis.upenn.edu/~chrjung/blog/monitor-argument-proof/">Proofs</a></h5>

<p>I will start off with a high level idea of the argument. Then, I&rsquo;ll go through the lemmas that are not specific to the settings. After, I&rsquo;ll finish off the argument for each setting.</p>

<p><br/>
<br/>
<br/></p>

<h3 id="the-monitor-argument">The Monitor Argument</h3>

<h4 id="what-are-we-trying-to-prove">What are we trying to prove?</h4>

<p>Let&rsquo;s first set up some notations and then state the goal of the monitor argument. Let our data domain be $\mathcal{X}$ and its distribution $\mathcal{P}$. Let $M: \mathcal{X}^n \to (\mathcal{X}^n \to [0,1])$ be some &lsquo;stable&rsquo; mechanism (the notion of &lsquo;stability&rsquo; will depend on the context). Given a set of $n$ i.i.d. points $S = \lbrace x_1, \dots, x_n\rbrace$, $M$ returns some function $q: \mathcal{X}^n \to [0,1]$. $M$ can be random. Then, we want to show that with high probability, $ q(S) $, where $q = M(S)$, is close to its expectation $E_{S \sim \mathcal{P}^n}\left[ q(S) \right]$.
More formally, we want
$$
\tag{1} \Pr_{S \sim \mathcal{P}^n, q = M(S)}\left( |q(S) - E_{S \sim \mathcal{P}^n} \left[ q(S) \right] | \ge \alpha \right) \le \beta,
$$
where we want $\alpha$ and $\beta$ to be small. Hopefully, the intuition as to why this is interesting and/or may be useful will be more obvious later when I actually go through the examples below.</p>

<p><details>
    <summary>Here are the original theorem statements that somewhat look like the above. (click here) </summary>
    <ul>
        <li>
            <div><a href="https://arxiv.org/abs/1511.02513"> Bassily et al. </a></div>
            <img src="../img/theorem_bassily.png">
        </li>
        <li>
            <div><a href="https://arxiv.org/abs/1812.09859"> Feldman et al. </a></div>
            <img src="../img/theorem_feldman.png" width="400">
        </li>
        <li>
            <div><a href="https://arxiv.org/abs/1701.03493"> Steinke et al. </a></div>
            <img src="../img/theorem_steinke.png">
        </li>
    </ul>
</details></p>

<p><br/></p>

<h3 id="applications">Applications</h3>

<h4 id="chernoff-bound">Chernoff Bound</h4>

<p>The Chernoff bound is at the heart of statistics and machine learning. At a high level, the bound characterizes how confident one can be in the quality of the empirical average in terms of estimating the true expected value. Or said another way, we want to show that the probability that the empirical average is $\alpha$ far away from its expected value ought to be small. For instance, in order to estimate the average height of some population, we can take some big enough random sample from the population and use its empirical average as an estimate for the population&rsquo;s average height. In machine learning, given many random (feature, label) pairs, we use a classifier&rsquo;s training error as a proxy to estimate how well it would do against the true distribution; these types of generelization bounds heavily rely on the Chernoff bound.</p>

<p>Let&rsquo;s try to rephrase this question of evaluating the quality of empirical average, using our notation above. In this case, $M$ deterministically outputs the average function $q_{avg}(S) = \frac{1}{n} \sum_{i=1}^n x_i.$ $M$ is &lsquo;stable&rsquo; because it always outputs $q_{avg}$ no matter what the input is.</p>

<p>Now, equation (1) can be written as
$$
\Pr_{S \sim \mathcal{P}^n}\left( |\frac{1}{n}\sum_{i=1}^n x_i - E_{S \sim \mathcal{P}^n} \left[ \frac{1}{n}\sum_{i=1}^n x_i \right] | \ge \alpha \right) \le \beta.
$$
We&rsquo;ve just dropped $M$ as $M$ no longer has any randomness and substituted in the definition of $q_{avg}$. This says that with probability at most $\beta$, the difference between the empirical average and the true average will be more than $\alpha$. Later, we&rsquo;ll show that in this case, fixing $\alpha$, we get $\beta = e^{-\Omega(\alpha^2 n)}$.</p>

<h4 id="adaptive-data-analysis">Adaptive Data Analysis</h4>

<p>Although the Chernoff Bound promises that for any <em>fixed</em> query $q: \mathcal{X}^n \to \mathbb{R}$, the empirical average will be close to its true expected value with high probability, the Chernoff bound no longer holds true for queries that are formed adaptively. When I say &lsquo;fixed&rsquo; queries, I mean questions that are formed before anything about the data is revealed. By contrast, adaptively formed queries are the questions that are formed after having gotten answers for previous questions.</p>

<p>Let&rsquo;s try to be a litte more precise here. Adaptive data analysis can be described by the interaction between a mechanism and a data analyst. A mechanism $G: \mathcal{X}^n \times (\mathcal{X}^n \to \mathbb{R}) \to \mathbb{R}$ is a stateful function which given some data set $S$ and some question $q$ will provide an answer $a$. Note that mechanism here is defined slightly differently than how we originally defined, but we&rsquo;ll show later that the difference doesn&rsquo;t really matter. And there&rsquo;s a data analyst $\Psi$ who is a stateful function which given answers to her previous questions will form a new question.</p>

<p><br/>
<figure class="image">
  <img src="../img/adaptive_data_analysis_interaction.png">
  <figcaption style="text-align: center">Interaction between the mechanism and the analyst</figcaption>
</figure>
<br/></p>

<p>What has been noted is that one can still get some sort of guarantee on the quality of the answer that the mechanism provides, if the mechanism $G$ is sample-accurate and is <a href="https://en.wikipedia.org/wiki/Differential_privacy">differentially private</a>. We say that mechanism $G$ is $(\alpha&rsquo;, \beta&rsquo;)$-sample accurate, if
$$
\Pr_{G}(|G(S,q) - q(S)| \ge \alpha&rsquo;) \le \beta&rsquo;.
$$
$G$ is $(\epsilon, \delta)$-differentially private, if for any two neighboring data sets $S$ and $S&rsquo;$, (i.e. they differ in only one element)
$$
\Pr_G(G(S,q) \in \xi) \le e^{\epsilon} \Pr_G(G(S&rsquo;,q) \in \xi) + \delta.
$$
One nice property of $(\epsilon, \delta)$-differentially privacy is its closure under post-processing. In other words, anything that is formed with the output of $G$ is still $(\epsilon,\delta)$-private. Therefore, it must be the case that any query $q$ which is formed with the outputs of mechanism $G$ must be differentially private, too.</p>

<p>Hence, to analyze $t$th query, we&rsquo;ll encapsulate the interaction between the mechanism and the data analyst up to round $t$ into $M^t$ and say $M^t(S) = q^t$. Once again, note that $M^t$ must be differentially private in terms $S$ by the post-processing of $G$ which is differential private.</p>

<p>Altogether, the monitor argument provides
$$
\Pr_{S \sim \mathcal{P}^n, q = M(S)}\left( |q(S) - E_{S \sim \mathcal{P}^n} \left[ q(S) \right] | \ge \alpha \right) \le \beta,
$$
and
$(\alpha&rsquo;, \beta&rsquo;)$-sample accuracy guarantees
$$
\Pr_{S \sim \mathcal{P}^n}\left( |q(S) - G(S,q) | \ge \alpha&rsquo; \right) \le \beta&rsquo;.
$$
Combining the above inequalities together along with the triangle inequality, we get that the answer that the mechanism $G$ provides is $(\alpha&rsquo; + \alpha)$ close to the true answer with probability $1-\beta-\beta&rsquo;$.
$$
\Pr_{S \sim \mathcal{P}^n, G}\left( |E_{S \sim \mathcal{P}^n} \left[ q(S) \right] - G(S,q) | \ge \alpha&rsquo; + \alpha \right) \le \beta + \beta&rsquo;.
$$</p>

<p>Awesome! We can still hope to answer questions that are formed adaptively and provide some accuracy guarantee for the mechanism. If you want to learn more about this topic, <a href="https://www.bu.edu/cs/profiles/adam-smith/">Adam Smith</a> and my advisor <a href="https://www.cis.upenn.edu/~aaroth/">Aaron Roth</a> taught a <a href="https://adaptivedataanalysis.com/">class</a> together, which has some great resources.</p>

<h4 id="uniformly-stable-algorithm">Uniformly stable algorithm</h4>

<p>In machine learning, one usually proves the generalization bound by combining the Chernoff bound and the complexity measure of the hypothesis class. For instance, the size of the hypothesis class may be infinite, but if the hypothesis&rsquo;s VC-dimension is $d$, there are at most $O(n^d)$ many ways to assign different labelings to $n$ points. Hence, we can simply take the union bound over these $O(n^d)$ possible labelings to argue that the training error must be close to its generealization error. However, sometimes this approach doesn&rsquo;t work and/or provides unsatisfactory bounds. One such example is Stochastic Gradient Descent.</p>

<p>However, just as before, stability comes in for the rescue! The stability of the learning algorithm may be used to prove its generalization bound. Let the data domain $\mathcal{X}$ consist of feature space $\mathcal{Z}$ and its binary label $\lbrace \pm 1\rbrace$ &ndash; that is, $\mathcal{X} = \mathcal{Z} \times \lbrace \pm 1\rbrace$. A learning algorithm $Alg: (\mathcal{Z} \times \lbrace \pm 1 \rbrace)^n \to (\mathcal{Z} \to \lbrace\pm 1 \rbrace )$ given $n$ (feature, label) pairs $S=\lbrace (z_1, y_1), \dots, (z_n ,y_n) \rbrace$ produces a hypothesis $f: \mathcal{Z} \to \lbrace\pm 1\rbrace$. There is some loss function $\ell: \lbrace \pm 1 \rbrace \times \lbrace \pm 1 \rbrace \to \mathbb{R}$ that measures the loss of predicting $\hat{y}$ when the true label is $y$. For instance, we may be interested in 0-1 loss, $\ell_{0-1}(\hat{y}, y) = 1[\hat{y} \neq y]$. Then, we can measure the training error of hypothesis $f$ as $L(f, \lbrace(z_1, y_1), \dots, (z_n, y_n) \rbrace) = \frac{1}{n}\sum_{i=1}^n \ell(f(z_i), y_i)$.</p>

<p>A learning algorithm $Alg$ is said to be $\gamma$-uniformly stable with respect to loss $\ell$ if for any neighboring dataset $S$ and $S&rsquo;$, we have that for every $(z, y) \in \mathcal{X}$,
$$
|\ell(f_S(z), y) - \ell(f_{S&rsquo;}(z), y)| \le \gamma,
$$<br />
where $f_S = Alg(S)$ and $f_{S&rsquo;} = Alg(S&rsquo;)$.</p>

<p>For the last time, let&rsquo;s try to rephrase the problem in terms of our notation. Every query will be parametrized by hypothesis $f$, and given some dataset $S$, it computes the overall loss of $f$ on $S$. In other words,
$$q_{f}(S) = L(f, S).$$</p>

<p>Now, let&rsquo;s have the mechanism $M$ take in some dataset $S$ and outputs $q_{Alg(S)}$. Then, rewriting (1) with these definitions yields
$$
\Pr_{S \sim \mathcal{P}^n, f = Alg(S)}\left( |L(f, S) - E_{S \sim \mathcal{P}^n} \left[ L(f, S) \right] | \ge \alpha \right) \le \beta.
$$</p>

<p>It basically says that training error on the dataset the classifier is trained on must be close to the true error with high probability.</p>

<p><br/></p>

<p>We&rsquo;ve seen some examples where the Monitor argument can be used to derive some concentration bounds. In the <a href="https://www.cis.upenn.edu/~chrjung/blog/monitor-argument-proof">next post</a>, we&rsquo;ll go over the overall proof structure of the Monitor argument.</p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-123514321-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
</body>

</html>
